---
title: "02-1-Understanding Deep Learning (Supervised learning)"

categories:
  - DeepLearning
tags:
  - [Python, DeepLearning]

toc: true
toc_sticky: true

date: 2024-09-01
last_modified_at: 2024-09-01
comments: true
---

# 제 2장 Supervised learning

*지도 학습 모델* 은 하나 이상의 입력을 하나 이상의 출력으로 매핑하는 방식을 정의합니다. 예를 들어, 입력은 중고 토요타 프리우스의 나이와 주행 거리일 수 있으며, 출력은 자동차의 예상 가격(달러)이 될 수 있습니다.

모델은 단지 수학적 방정식일 뿐이며, 이 방정식을 통해 입력이 전달되면 출력을 계산합니다. 이를 *추론(inference)* 이라고 합니다. 모델 방정식은 또한 *매개변수(parameter)* 를 포함합니다. 서로 다른 매개변수 값은 계산 결과에 변화를 줍니다. 모델 방정식은 입력과 출력 간의 가능한 관계들을 설명하며, 매개변수는 특정한 관계를 지정합니다.

모델을 *훈련*하거나 *학습*할 때, 우리는 입력과 출력 간의 실제 관계를 설명하는 매개변수를 찾습니다. 학습 알고리즘은 입력/출력 쌍으로 이루어진 학습 데이터를 사용하고, 입력이 가능한 한 정확하게 해당 **출력을 예측할 때까지 매개변수를 조정합니다.** 

이 장의 목표는 이러한 아이디어를 확장하는 것입니다. 먼저, 이 프레임워크를 더 공식적으로 설명하고 일부 표기법을 소개합니다. 그런 다음, 입력과 출력 간의 관계를 직선으로 설명하는 간단한 예제를 다룹니다. 이 선형 모델은 친숙하고 시각화하기 쉽지만, 지도 학습의 주요 개념을 모두 설명하는 예입니다.

## 2.1 Supervised learning overview
지도 학습에서 우리는 입력 $\mathbf{x}$를 받아서 출력 $\mathbf{y}$를 예측하는 모델을 구축하는 것을 목표로 합니다. 간단하게 하기 위해, 우리는 입력 $\mathbf{x}$와 출력 $\mathbf{y}$가 미리 정해진 고정 크기의 벡터라고 가정하며, 각 벡터의 요소들은 항상 동일한 방식으로 정렬된다고 가정합니다. 예를 들어, 위의 프리우스 예시에서 입력 $\mathbf{x}$는 항상 차량의 나이와 그 다음으로 주행 거리를 포함하며, 이 순서로 정렬됩니다. 이를 구조화된 또는 표 형식 데이터라고 합니다.

예측을 하기 위해서는, 입력 $\mathbf{x}$를 받아 $\mathbf{y}$를 반환하는 모델 $f[\cdot]$가 필요하며, 다음과 같이 나타낼 수 있습니다:

$$
\mathbf{y} = f[\mathbf{x}].
$$

입력 $\mathbf{x}$로부터 예측 $\mathbf{y}$를 계산할 때, 이를 *추론*이라고 합니다.

> 그냥 간단하게 주행거리: x를 넣으면 예상 중고가격: y가 나오는 어떤 함수가 필요하다는 내용이당~~

모델은 고정된 형식을 가진 단순한 수학적 방정식일 뿐입니다. 이 방정식은 입력과 출력 간의 다양한 관계를 나타냅니다. 모델은 또한 매개변수 $\phi$를 포함합니다. 매개변수의 선택은 입력과 출력 간의 특정 관계를 결정하므로, 실제로는 다음과 같이 작성해야 합니다:

$$
\mathbf{y} = f[\mathbf{x}, \phi].
$$

모델을 *학습*하거나 *훈련*한다고 말할 때, 우리는 입력으로부터 합리적인 출력을 생성하는 매개변수 $\phi$를 찾으려고 시도하는 것입니다. 우리는 $\{ \mathbf{x}_i, \mathbf{y}_i \}$와 같은 입력과 출력 예시 쌍 $I$를 포함하는 훈련 데이터셋을 사용하여 이 매개변수를 학습합니다. 각 훈련 입력을 가능한 한 정확하게 관련 출력에 매핑하는 매개변수를 선택하는 것을 목표로 합니다. 이 매핑에서 발생하는 손실 $L$로 측정합니다. 이는 모델이 매개변수 $\phi$에 대해 훈련 출력 $\mathbf{y}_i$를 훈련 입력 $\mathbf{x}_i$로부터 얼마나 잘 예측하는지를 요약하는 스칼라 값입니다.

우리는 손실을 이 매개변수들의 함수 $L[\phi]$로 취급할 수 있습니다. 모델을 훈련시킬 때, 우리는 이 손실 함수(loss function):

$$
\hat{\phi} = \arg \min_{\phi} \left[ L[\phi] \right]
$$

를 최소화하는 매개변수 $\hat{\phi}$를 찾고 있습니다.

이 최소화 후 손실이 작다면, 우리는 훈련 입력 $\mathbf{x}_i$로부터 훈련 출력 $\mathbf{y}_i$를 정확하게 예측하는 모델 매개변수를 찾은 것입니다.

모델을 훈련한 후, 이제 성능을 평가해야 합니다. 모델을 별도의 테스트 데이터에 대해 실행하여 훈련 중에 관찰하지 않은 예시에 대해 얼마나 잘 일반화하는지를 확인합니다. 성능이 적절하다면, 우리는 모델을 배포할 준비가 된 것입니다. 

> 음 내용이 많은데 $y=ax+b$라고 가정하면 parameter a, b를 찾는 loss function $L[\phi]$를 정의해서 x, y 데이터로 a,b의 값을 찾는 과정이당~~ (뒤에서 자세하게 설명할 듯~~)


## 2.2 선형 회귀 예제

이제 이러한 아이디어를 간단한 예제를 통해 구체화해 보겠습니다. 우리는 단일 입력 $x$로부터 단일 출력 $y$를 예측하는 모델 $y = f[x, \phi]$를 고려합니다. 그 후 손실 함수를 개발하고, 마지막으로 모델 훈련에 대해 논의합니다.

### 2.2.1 1D linear regression model
1차원 선형 회귀 모델(1D linear regression model)은 입력 $x$와 출력 $y$ 간의 관계를 직선으로 설명합니다:

$$
y = f[x, \phi] \\
  = \phi_0 + \phi_1 x.
$$

![](https://private-user-images.githubusercontent.com/44514383/363462367-6862b3f2-a10a-4a28-89bd-4084c8573d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNzEwNjgsIm5iZiI6MTcyNTE3MDc2OCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjIzNjctNjg2MmIzZjItYTEwYS00YTI4LTg5YmQtNDA4NGM4NTczZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA2MDYwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTVmZmRmNjhlMzBiYjYzYTBjNjA1NDBkNDJhMmU5OTI4ODg4NzE0NmQ1NzljOTNmNTUzZTY0NDUzODU5Y2U0MGQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QS6dys4ymOubEAj0gQtfVA46OVTRv9oDRtxAFlXFBL0)

이 모델은 두 개의 매개변수 $\phi = [\phi_0, \phi_1]$를 가지며, 여기서 $\phi_0$는 직선의 y절편이고 $\phi_1$은 기울기입니다. y절편과 기울기의 선택에 따라 입력과 출력 간의 관계가 달라집니다(그림 2.1 참조). 따라서 방정식 2.4는 가능한 입력-출력 관계(모든 가능한 직선들)의 집합을 정의하며, 매개변수의 선택이 특정 직선을 결정합니다.

### 2.2.2 Loss
이 모델에서 훈련 데이터셋(그림 2.2a)은 $I$개의 입력/출력 쌍 $\{x_i, y_i\}$로 구성됩니다. 그림 2.2b–d는 세 가지 매개변수 집합으로 정의된 세 개의 직선을 보여줍니다. 그림 2.2d의 녹색 직선은 다른 두 개의 직선보다 데이터에 훨씬 더 가깝기 때문에 데이터를 더 정확하게 설명합니다. 

그러나 어떤 매개변수 $\phi$가 다른 것보다 나은지를 결정하기 위한 원칙적인 접근이 필요합니다. 이를 위해, 모델과 데이터 간의 차이 정도를 정량화하는 각 매개변수에 수치값을 할당합니다. 이 값을 *손실*이라고 부르며, 손실이 낮을수록 더 나은 적합을 의미합니다.
> 즉 모델에서 예측한 값이랑 실제 값이랑 차이를 줄인다고 보면 된다~~

![](https://private-user-images.githubusercontent.com/44514383/363462866-e28ee50c-2aaf-4811-8e0f-3bad03af8ab2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNzE2ODQsIm5iZiI6MTcyNTE3MTM4NCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjI4NjYtZTI4ZWU1MGMtMmFhZi00ODExLThlMGYtM2JhZDAzYWY4YWIyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA2MTYyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkwYTEzZmEyMzZjZTI2NjA2NjY0MDBiMTdkZDQzZTE4Zjc5YzNhYjg0ZmUwNjVlMTMxZDZkZDNiY2NiZGI3N2YmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.XM5uAt-5SehuU8SxhK4nWmCG7_7weL-5-G3DoJZ0inU)

![](https://private-user-images.githubusercontent.com/44514383/363462898-9a79df3c-204f-4e22-bd44-57b7c8c17a1c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNzE2ODQsIm5iZiI6MTcyNTE3MTM4NCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjI4OTgtOWE3OWRmM2MtMjA0Zi00ZTIyLWJkNDQtNTdiN2M4YzE3YTFjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA2MTYyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNmZmFlYzJjMzNkNmIzM2M3NWZhMmY5MDdiNmFhMGRhNmQ1MTg0MDdkNTIzYjU2ODcwZmU4MTAyNmIxZGNiZDMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.88Nl42BerpXtOTL5G8omOU1N7t0mPVGk9ZIa9fRFdkc)

불일치는 모델 예측 $f[x_i, \phi]$ (즉, $x_i$에서의 직선의 높이)와 실제 출력 $y_i$ 간의 차이로 포착됩니다. 이러한 차이는 그림 2.2b–d에서 주황색 점선으로 표시됩니다. 우리는 이 불일치의 총합을 훈련 오류 또는 손실로 정량화하며, 모든 $I$개의 훈련 쌍에 대해 이 차이들의 제곱합으로 표현됩니다:

$$
L[\phi] = \sum_{i=1}^{I} \left( f[x_i, \phi] - y_i \right)^2 \\
        = \sum_{i=1}^{I} \left( \phi_0 + \phi_1 x_i - y_i \right)^2.
$$

최적의 매개변수는 이 식을 최소화하므로, 이를 **최소제곱 손실(least-squares loss)** 이라고 합니다. 

손실 $L$은 매개변수 $\phi$의 함수이며, 모델 적합도가 나쁘면(그림 2.2b, c) 더 커지고, 모델 적합도가 좋으면(그림 2.2d) 더 작아집니다. 이 관점에서 우리는 $L[\phi]$를 *손실 함수* 또는 *비용 함수*라고 부릅니다. 목표는 이 값을 최소화하는 매개변수 $\hat{\phi}$를 찾는 것입니다:

$$
\hat{\phi} = \arg\min_{\phi} \left[ L[\phi] \right] \\
          = \arg\min_{\phi} \left[ \sum_{i=1}^{I} \left( f[x_i, \phi] - y_i \right)^2 \right] \\
          = \arg\min_{\phi} \left[ \sum_{i=1}^{I} \left( \phi_0 + \phi_1 x_i - y_i \right)^2 \right].
$$

여기서 매개변수는 두 개($\phi_0$ y절편과 $\phi_1$ 기울기)뿐이므로, 우리는 모든 값의 조합에 대해 손실을 계산하고, 그림 2.3에서 손실 함수를 표면으로 시각화할 수 있습니다. "최적의" 매개변수는 이 함수의 최소 지점에 위치하게 됩니다.