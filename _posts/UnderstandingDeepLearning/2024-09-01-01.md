---
title: "01-1-Understanding Deep Learning (Introduction)"

categories:
  - DeepLearning
tags:
  - [Python, DeepLearning]

toc: true
toc_sticky: true

date: 2024-09-01
last_modified_at: 2024-09-01
---
> Hㅏ.... 책이 좋은데 TMI가 좀 많아서 좀 요약하면서 해보겠습니다. 
# 제1장 Introduction

## 소개

- 이 책은 인공지능(AI)과 머신 러닝, 특히 딥러닝의 기본 원리를 설명합니다.
- 머신 러닝은 AI의 하위 집합으로, 데이터에 기반하여 결정을 내리는 방법을 학습합니다.
- 딥러닝은 딥 뉴럴 네트워크를 활용한 머신 러닝 모델로, 다양한 실생활 응용 프로그램에서 사용됩니다.
- 이 책은 이론적이기보다는 아이디어를 설명하며, 실용적인 코드보다는 개념 이해에 중점을 둡니다.
- 마지막으로, 딥러닝의 윤리적 영향과 AI의 미래에 대해 간단히 논의합니다.

![](https://private-user-images.githubusercontent.com/44514383/363460263-104a1842-e3d4-4072-922b-92ead2c8ff81.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNjc2MDgsIm5iZiI6MTcyNTE2NzMwOCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjAyNjMtMTA0YTE4NDItZTNkNC00MDcyLTkyMmItOTJlYWQyYzhmZjgxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA1MDgyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM5YjMwNzVhZDVkOTNlMTRlNGI3NzhkMzNlNzYyMmEyZDI0YmJlYzZmYjdkMDkxMjBmYjM0YTQyNTA2M2Y1MDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.qd7fuLcRKRTK8I0IctN_3vGtoIcVLRY3c6lDQC4yanU)



## 1.1 Supervised learning
### 1.1.1 Regression and classification problems

이 그림(1.2)은 여러 회귀 및 분류 문제를 설명하는데 내용이 너무 많다;;

**간단하게 요약하면** 

문장, 음성 파일, 이미지 등을 숫자 벡터로 인코딩 &rarr; 블랙박스 &rarr; 벡터 출력 &rarr; 사람이 읽을수 있는 형식으로 다시 디코딩
이다!!

![](https://private-user-images.githubusercontent.com/44514383/363460262-e3cf03a5-d53e-4803-a6e8-f3c060f3dace.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNjc2MDgsIm5iZiI6MTcyNTE2NzMwOCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjAyNjItZTNjZjAzYTUtZDUzZS00ODAzLWE2ZTgtZjNjMDYwZjNkYWNlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA1MDgyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMDQ5NGI3Yjg0NTc5NGNkZmRhYTQ4YjE4MzZlZGU0ODRkOTA4ZmJjMDY1YWMyZmJkNjlkZGZkOTI2MzQ5NmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Kzdxg_Qpw0rZ1mWBjYUsSl7lvxkv_p3zmbOocWX7VA0)


- **그림 1.2a의 모델:** 주택의 크기와 침실 수와 같은 입력 특성에 기반해 주택의 가격을 예측합니다. 이는 연속적인 숫자를 반환하므로 회귀 문제입니다.
- **그림 1.2b의 모델:** 분자의 화학 구조를 입력으로 받아 동결점과 비등점을 예측합니다. 이는 여러 숫자를 예측하므로 다변량 회귀 문제입니다.
- **그림 1.2c의 모델:** 레스토랑 리뷰를 입력받아 긍정적인지 부정적인지를 예측합니다. 이는 이진 분류 문제로, 모델이 입력을 두 개의 범주 중 하나로 할당하려고 시도하기 때문입니다.
- **그림 1.2d와 1.2e의 모델:** 다중 클래스 분류 문제를 나타내며, 첫 번째 경우는 오디오 파일을 입력으로 받아 음악 장르를 예측하고, 두 번째 경우는 이미지를 입력으로 받아 물체의 종류를 예측합니다. 각 경우에서, 모델은 N개의 범주에 대한 확률을 포함하는 크기 N의 벡터를 반환합니다.


### 1.1.2 Inputs 
그림 1.2에서 입력 데이터는 다양하게 나타납니다

- **주택 가격 예측 예시**
    
    고정된 길이의 벡터. 이는 순서가 중요하지 않은 표 형식 데이터(tabular data), 입력 순서를 변경해도 모델 예측이 동일하게 유지
- **레스토랑 리뷰 예시**

  가변 길이의 텍스트로, 입력 순서가 중요 예를 들어 “내 아내가 치킨을 먹었다”와 “치킨이 내 아내를 먹었다”는 전혀 다른 의미 이 텍스트는 모델에 전달되기 전에 숫자 형식으로 인코딩됩니다. 
- **음악 분류 예시**

  고정된 크기의 입력 벡터(10초 클립)가 매우 고차원적일 수 있습니다. 디지털 오디오는 441,000개의 정수로 구성됩니다. 따라서 지도 학습 모델은 큰 입력을 처리할 수 있어야 합니다.
- **이미지 분류 예시**

  입력 벡터는 각 픽셀의 RGB 값을 연결하여 구성되며, 또한 공간적 구조를 포함합니다. **예를 들어, 서로 위아래에 위치한 두 픽셀은 서로 밀접한 관계가 있습니다**.
- **분자의 동결점 및 비등점 예측 예시**

  분자는 다양한 수의 원자를 포함할 수 있고, 이는 여러 방식으로 연결될 수 있습니다. 이 경우, 모델은 분자의 기하학적 구조와 구성 원자를 함께 처리해야 합니다.


### 1.1.3 Machine learning models 
이 방정식은 입력을 출력으로 매핑하는 여러 곡선 중 하나로, 훈련 데이터를 사용해 선택됩니다.

훈련 데이터는 입력/출력 쌍으로 구성되며, 모델이 이 데이터를 가장 잘 설명하는 곡선을 찾습니다.

그림 1.2의 모델들도 훈련을 위해 레이블이 지정된 입출력 쌍이 필요합니다. 예를 들어, 음악 분류 모델은 각 오디오 클립의 장르를 사람이 식별한 다수의 오디오 클립이 필요합니다. 이러한 입출력 쌍은 훈련 과정에서 교사 또는 감독자의 역할을 하며, 이로 인해 ’지도 학습(supervised learning)’이라는 용어가 생겨났습니다.

즉 데이터를 입출력 데이터를 넣어주면 **지도 학습(supervised learning)** 이다. 

> 더 간단하게 내가 이해한 점을 보면 그냥 입출력 데이터는 사람이 넣어주야되고 그 데이터를 잘 표현하는 함수를 찾는 과정을 **지도학습** 이라고 이해하면 될 것 같다. 


> **trash in trash out** 이라는 얘기가 있는데 결국 이상한 데이터를 넣어주면 이상한 함수를 찾기 때문에 넣어주튼 데이터가 그만큼 중요해진다~~


### 1.1.4 Deep neural networks
이 책은 특히 유용한 기계 학습 모델인 심층 신경망(Deep Neural Networks)에 대해 다룹니다. 

심층 신경망은 입력과 출력 사이의 매우 광범위한 관계를 나타낼 수 있는 방정식으로, 훈련 데이터를 가장 잘 설명하는 관계를 쉽게 찾을 수 있습니다. 
- 단일 실수(회귀)
- 다수의 숫자(다변량 회귀)
- 여러 클래스에 대한 확률(이진 및 다중 클래스 분류)

을 출력할 수 있다. 

### 1.1.5 Structured outputs 
![](https://private-user-images.githubusercontent.com/44514383/363460259-666c29c5-8f2a-4309-a84a-39687a2e2a25.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNjc2MDgsIm5iZiI6MTcyNTE2NzMwOCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjAyNTktNjY2YzI5YzUtOGYyYS00MzA5LWE4NGEtMzk2ODdhMmUyYTI1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA1MDgyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWE0NGQxMjkxYTM1ZjE2NTI0ZGFlMTA0Y2Y5MTY3MGYyZjg1NGU4MWNlZjhjNWVkOWQ4NmY4MWRjZjkxYzQyN2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.1xIMBJ9_9T1H8qhg0UGT45Z70r5D6ZXotVW_jKIM-7k)

1. **Figure 1.4a**: 다변량 이진 분류 모델이 입력 이미지의 각 픽셀을 소나 배경으로 분류하는 예시를 보여줍니다.
2. **Figure 1.4b**: 다변량 회귀 모델이 입력 이미지에서 각 픽셀의 깊이를 예측하는 예시를 보여줍니다.
3. 두 경우 모두 출력이 고차원적이며, 출력 구조가 입력 구조와 밀접하게 연관되어 있습니다.
4. **Figure 1.4c-e**: 출력 구조가 입력과 덜 연관된 복잡한 모델들을 보여줍니다.
5. **Figure 1.4c**: 입력이 오디오 파일이고, 출력이 해당 파일의 텍스트로 전사된 단어들입니다.
6. **Figure 1.4d**: 영어 텍스트를 프랑스어로 번역하는 모델입니다.
7. **Figure 1.4e**: 설명 텍스트를 기반으로 이미지를 생성하는 어려운 모델입니다.


## 1.2 Unsupervised learning 
출력 레이블이 없는 입력 데이터로부터 모델을 구축하는 것을 비지도 학습이라고 합니다. 출력 레이블이 없다는 것은 **“감독”** 이 없음을 의미합니다. 입력에서 출력으로의 매핑을 학습하는 대신, 데이터의 구조를 설명하거나 이해하는 것이 목표입니다. 지도 학습의 경우와 마찬가지로, 비지도 학습의 데이터도 매우 다양한 특성을 가질 수 있습니다. 데이터는 다음과 같을 수 있습니다:

- 이산적이거나 연속적일 수 있음
- 저차원 또는 고차원일 수 있음
- 일정한 길이 또는 가변적인 길이를 가질 수 있음

비지도 학습의 주요 초점은 레이블이 지정된 결과 없이 데이터 자체 내에서 패턴이나 구조를 발견하는 것입니다. 
> 이산적이다는 것은 연속적이지 않는 데이터를 얘기한다. (E.g: 슈팅횟수, 단속 횟수 등등)

### 1.2.1 Generative models (생성모델)
![](https://private-user-images.githubusercontent.com/44514383/363460264-00584c4c-7580-4bb1-bb60-0e0e5521cd8b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNjc2MDgsIm5iZiI6MTcyNTE2NzMwOCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjAyNjQtMDA1ODRjNGMtNzU4MC00YmIxLWJiNjAtMGUwZTU1MjFjZDhiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA1MDgyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg4OWQyNTkwMTMzYWJhZWM3YWU4OTdhYzkxNGExZTJjODQwMWQxNjFjOTI3MDk5ZjZmMzkyNTllNDQ4NDI3ZDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9BJft4KFAw7ZAOItz7dN6ABDXwJ6gFKhdGCHC_qWGAU)

![](https://private-user-images.githubusercontent.com/44514383/363460260-feb6e821-a104-4e56-81bb-037e19a1cdd8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNjc2MDgsIm5iZiI6MTcyNTE2NzMwOCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjAyNjAtZmViNmU4MjEtYTEwNC00ZTU2LTgxYmItMDM3ZTE5YTFjZGQ4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA1MDgyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ3ZThiNDg4ZmZiZTdmNmZlZmYzMjFmNDZlODU1ZGRlNWQzOTk5MzM3ZjdjZWRjYjc4OTVlOGJlZWU5MjM2NjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.KdZKKAt6t4NhJyoqeiTNOtx8Mp5waMd243xviogoQcs)

이 책은 **생성적 비지도 학습** 모델에 중점을 두고 있습니다. 
이러한 모델은 훈련 데이터와 **통계적으로 구별할 수 없는 새로운 데이터 예제** 를 합성하는 것을 학습합니다. 

### 1.2.2 Latent variables 

![](https://private-user-images.githubusercontent.com/44514383/363460261-11a28c5a-a25a-4caa-ba6a-ad9f6794d439.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUxNjc2MDgsIm5iZiI6MTcyNTE2NzMwOCwicGF0aCI6Ii80NDUxNDM4My8zNjM0NjAyNjEtMTFhMjhjNWEtYTI1YS00Y2FhLWJhNmEtYWQ5ZjY3OTRkNDM5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAxVDA1MDgyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI1YzQ0MTZlNGE0M2E1OWQxOThkNzllZGFiZjQ5Yzk2YTUwZTcwN2QwMmVhZjI2ZWIyYzhkNmM5MTA2NmQwNGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.hyU0lqgW8th1kKUDD02BLTEtNZGTOcfyaYu6bgw_qq8)

일부(하지만 모든 것은 아님) 생성 모델들은 데이터가 관찰된 변수의 수보다 더 낮은 차원을 가질 수 있다.
> 그냥 어떠한 데이터가 있으면 더 적은 벡터(Latent Vector)로 표현할 수 있다는 내용인 것 같다.  

이로 인해 각 데이터 예제를 더 적은 수의 잠재 변수로 설명할 수 있다. 
여기서 딥러닝의 역할은 이러한 잠재 변수와 데이터 사이의 매핑을 설명하는 것입니다. 잠재 변수들은 일반적으로 설계에 의해 단순한 확률 분포를 가집니다. 이 분포에서 샘플링한 결과를 딥러닝 모델을 통해 전달함으로써 새로운 샘플을 생성할 수 있습니다(그림 1.10).
> 음 쉽게 생각하면 예를 들어 입대시 남성 평균키를 정규분포로 학습시킨다고 가정하면 평균과 표준편차만 찾으면 된다. 그래서 평균 175 표준편차 10이라는 것을 학습했으면 이를 기반으로 np.random.normal(175, 10, 1000)으로 1000명의 가짜 남성 키 데이터를 **생성하면** 이 데이터는 실제 남성 키 데이터랑 분포가 비슷할 것이당~~~


이러한 모델들은 실제 데이터를 조작하는 새로운 방법들을 제시합니다. 예를 들어, 두 개의 실제 예제를 뒷받침하는 잠재 변수를 찾는다고 가정해 보겠습니다. 우리는 이 예제들 사이의 잠재 표현을 보간(interpolation)하고, 중간 위치를 데이터 공간으로 다시 매핑함으로써 이 예제들 사이를 보간할 수 있습니다(그림 1.11).
> 그림 1.11에서 오른쪽 사진과 왼쪽 사진 2개만 있으면 오른쪽에서 왼쪽 사진으로 가는 그 가운데 사진들 보간할 수 있다는 내용이당

### 1.2.3 Connecting supervised and unsupervised learning
잠재 변수를 사용하는 생성 모델은 출력에 구조가 있는 지도 학습 모델에도 이점을 제공할 수 있습니다(그림 1.4 참고). 

예를 들어, 캡션에 해당하는 이미지를 예측하는 방법을 학습한다고 가정해 보겠습니다. 텍스트 입력을 이미지로 직접 매핑하는 대신, 텍스트를 설명하는 잠재 변수와 이미지를 설명하는 잠재 변수 간의 관계를 학습할 수 있습니다.

이 접근 방식은 세 가지 장점을 가집니다. 
- 첫째, 입력과 출력이 더 낮은 차원을 가지기 때문에 이 매핑을 학습하기 위해 더 적은 수의 텍스트/이미지 쌍이 필요할 수 있습니다. 
  > ram을 아낄 수 있다. 
- 둘째, 그럴듯한 이미지를 생성할 가능성이 더 높아집니다. 잠재 변수의 모든 합리적인 값은 그럴듯한 예제처럼 보이는 무언가를 생성할 것입니다. 
  > 생성이 잘된다?
- 셋째, 두 잠재 변수 집합 간의 매핑이나 잠재 변수에서 이미지로의 매핑에 무작위성을 도입하면, 해당 캡션으로 잘 설명될 수 있는 여러 이미지를 생성할 수 있습니다(그림 1.12).


## 1.3 Reinforcement learning (강화학습)

머신러닝의 마지막 영역은 강화 학습입니다. 이 패러다임은 특정 시간 간격마다 특정 행동을 수행할 수 있는 환경에서 활동하는 에이전트(agent)의 개념을 도입합니다. 이러한 행동은 시스템의 상태를 변화시키지만, 그 변화가 반드시 결정론적(deterministic)인 것은 아닙니다. 행동을 취하는 것은 또한 보상을 생성할 수 있으며, 강화 학습의 목표는 에이전트가 평균적으로 높은 보상으로 이어지는 행동을 선택하도록 학습하는 것입니다.

> 게임이라고 이해하면 된다. 만드는 사람은 agent가 어떻게 해동할지에 따라서 보상을 정하면 보상을 최대화하게 행동한다~~

하나의 복잡성은 보상이 행동이 이루어진 직후가 아니라 일정 시간이 지난 후에 발생할 수 있다는 점입니다. 따라서 보상을 특정 행동과 연관시키는 것은 간단하지 않습니다. 이는 **시간적 신용 할당 문제(temporal credit assignment problem)** 로 알려져 있습니다. 
> 보상은 agent가 행동을 한 다음에 보상이 와서 지연이 있다는 내용이다. 

에이전트가 이미 적절한 보상을 받는 방법을 학습했을 때, 이 전략을 따를 것인가(이미 알고 있는 것을 활용할 것인가), 아니면 다른 행동을 시도하여 더 나은 결과를 얻을 수 있는지 탐색할 것인가(다른 기회를 탐구할 것인가)라는 문제를 고려해야 합니다.
> 기존은 행동에서 보상이 높은 행동을 계속 할건지 탐험을 할건지 정해야된다~~


## 1.4 Ethics
> 가볍게 Pass~~



# 결론
여기서는 크게 두가지에 대해서 설명했다.
- **지도학습**
- **비지도학습**


여기서 앞으로 딥러닝을 어떻게 설명할지 얘기했는데 한번 읽어보면 좋은 내용들이었당 이제 Chapter 2부터 제대로 시작해보자~~

